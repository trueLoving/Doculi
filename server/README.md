# Doculi Server

Doculiçš„åç«¯æœåŠ¡ï¼ŒåŸºäºMCPï¼ˆModel Context Protocolï¼‰åè®®ï¼Œæä¾›æœ¬åœ°LLMé©±åŠ¨çš„æ–‡æ¡£è½¬æ¢æœåŠ¡ã€‚

## ğŸ—ï¸ æ¶æ„

```
Frontend (TypeScript) 
    â†“ HTTP/WebSocket
MCP Server (Python FastAPI)
    â†“ APIè°ƒç”¨
Local LLM (Ollama/LM Studio)
    â†“ å¤„ç†ç»“æœ
MCP Server
    â†“ å“åº”
Frontend
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœ¬åœ°å¼€å‘

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨æœåŠ¡
python src/mcp_server.py

# æˆ–ä½¿ç”¨å¯åŠ¨è„šæœ¬
./start_server.sh
```

### 2. Dockeréƒ¨ç½²

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f mcp-server
```

### 3. æ‰‹åŠ¨å®‰è£…Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull llama3.1:8b

# å¯åŠ¨Ollama
ollama serve
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
server/src/
â”œâ”€â”€ mcp_server.py          # FastAPIä¸»æœåŠ¡å™¨
â”œâ”€â”€ mcp_tools.py           # MCPå·¥å…·ç®¡ç†
â”œâ”€â”€ llm_client.py          # LLMå®¢æˆ·ç«¯
â”œâ”€â”€ document_converter.py  # æ–‡æ¡£è½¬æ¢å™¨
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile             # Dockeré…ç½®
â”œâ”€â”€ docker-compose.yml     # Docker Composeé…ç½®
â”œâ”€â”€ start_server.sh        # å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md              # è¯´æ˜æ–‡æ¡£

server/
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile             # Dockeré…ç½®
â”œâ”€â”€ docker-compose.yml     # Docker Composeé…ç½®
â”œâ”€â”€ start_server.sh        # å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md              # è¯´æ˜æ–‡æ¡£
```

## ğŸ”§ é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# MCPæœåŠ¡å™¨é…ç½®
MCP_HOST=0.0.0.0
MCP_PORT=8000
OLLAMA_HOST=0.0.0.0
OLLAMA_PORT=11434

# æ–‡ä»¶å¤„ç†é…ç½®
MAX_FILE_SIZE=100MB
UPLOAD_DIR=./uploads
TEMP_DIR=./temp
LOG_DIR=./logs
```

### é…ç½®æ–‡ä»¶ (config.py)

```python
MCP_CONFIG = {
    "host": "0.0.0.0",
    "port": 8000,
    "llm_endpoint": "http://localhost:11434",
    "timeout": 30,
    "max_file_size": "100MB"
}
```

## ğŸ“¡ APIæ¥å£

### å¥åº·æ£€æŸ¥

```bash
GET /health
```

### æœåŠ¡å™¨çŠ¶æ€

```bash
GET /api/status
```

### MCPåè®®æ¥å£

```bash
POST /mcp
Content-Type: application/json

{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/list",
  "params": {}
}
```

### æ–‡æ¡£è½¬æ¢

```bash
POST /api/convert
Content-Type: multipart/form-data

file: [æ–‡ä»¶]
target_format: pdf|docx|txt|html
preserve_formatting: true|false
extract_tables: true|false
extract_images: true|false
language: auto|zh|en
```

### æ¨¡å‹ç®¡ç†

```bash
# è·å–å¯ç”¨æ¨¡å‹
GET /api/models

# ä¸‹è½½æ¨¡å‹
POST /api/models/{model_name}/download
```

## ğŸ› ï¸ MCPå·¥å…·

### 1. convert_document
æ–‡æ¡£æ ¼å¼è½¬æ¢å·¥å…·

**å‚æ•°:**
- `file_path`: æ–‡ä»¶è·¯å¾„
- `target_format`: ç›®æ ‡æ ¼å¼ (pdf, docx, txt, html)
- `preserve_formatting`: æ˜¯å¦ä¿ç•™æ ¼å¼
- `extract_tables`: æ˜¯å¦æå–è¡¨æ ¼
- `extract_images`: æ˜¯å¦æå–å›¾ç‰‡
- `language`: è¯­è¨€è®¾ç½®

### 2. ocr_document
OCRæ–‡æœ¬æå–å·¥å…·

**å‚æ•°:**
- `file_path`: æ–‡ä»¶è·¯å¾„
- `language`: OCRè¯­è¨€ä»£ç 

### 3. analyze_document
æ–‡æ¡£åˆ†æå·¥å…·

**å‚æ•°:**
- `file_path`: æ–‡ä»¶è·¯å¾„
- `analysis_type`: åˆ†æç±»å‹ (structure, content, metadata, all)

## ğŸ” æ—¥å¿—

æ—¥å¿—æ–‡ä»¶ä½ç½®: `logs/mcp_server.log`

æ—¥å¿—çº§åˆ«:
- INFO: ä¸€èˆ¬ä¿¡æ¯
- WARNING: è­¦å‘Šä¿¡æ¯
- ERROR: é”™è¯¯ä¿¡æ¯
- DEBUG: è°ƒè¯•ä¿¡æ¯

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **OllamaæœåŠ¡æ— æ³•å¯åŠ¨**
   ```bash
   # æ£€æŸ¥ç«¯å£å ç”¨
   lsof -i :11434
   
   # é‡å¯æœåŠ¡
   ollama serve
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # æ¸…ç†ç¼“å­˜
   ollama rm llama3.1:8b
   ollama pull llama3.1:8b
   ```

3. **MCPæœåŠ¡å™¨è¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
   curl http://localhost:8000/health
   
   # æŸ¥çœ‹æ—¥å¿—
   tail -f logs/mcp_server.log
   ```

### æ€§èƒ½ä¼˜åŒ–

1. **Ollamaä¼˜åŒ–**
   ```bash
   export OLLAMA_NUM_PARALLEL=2
   export OLLAMA_MAX_LOADED_MODELS=1
   export OLLAMA_FLASH_ATTENTION=1
   ```

2. **GPUåŠ é€Ÿ**
   ```bash
   export CUDA_VISIBLE_DEVICES=0
   ```

## ğŸ”’ å®‰å…¨è€ƒè™‘

1. **ç½‘ç»œå®‰å…¨**
   - æœåŠ¡å™¨ä»…ç›‘å¬æœ¬åœ°æ¥å£
   - ä½¿ç”¨é˜²ç«å¢™é™åˆ¶å¤–éƒ¨è®¿é—®
   - ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨HTTPS

2. **æ•°æ®éšç§**
   - æ‰€æœ‰å¤„ç†åœ¨æœ¬åœ°å®Œæˆ
   - ä¸å‘å¤–éƒ¨æœåŠ¡å‘é€æ•°æ®
   - ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†

3. **è®¿é—®æ§åˆ¶**
   - å¯é…ç½®APIå¯†é’¥éªŒè¯
   - æ–‡ä»¶å¤§å°é™åˆ¶
   - è¯·æ±‚é¢‘ç‡é™åˆ¶

## ğŸ“Š ç›‘æ§

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

### çŠ¶æ€ç›‘æ§

```bash
curl http://localhost:8000/api/status
```

### æ—¥å¿—ç›‘æ§

```bash
tail -f logs/mcp_server.log
```

## ğŸš€ éƒ¨ç½²

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

1. **ä½¿ç”¨Docker Compose**
   ```bash
   docker-compose -f docker-compose.prod.yml up -d
   ```

2. **ä½¿ç”¨systemd (Linux)**
   ```bash
   sudo systemctl enable docuSynapse-server
   sudo systemctl start docuSynapse-server
   ```

3. **ä½¿ç”¨PM2 (Node.js)**
   ```bash
   pm2 start mcp_server.py --name docuSynapse-server
   pm2 save
   pm2 startup
   ```

## ğŸ“ˆ æ‰©å±•

### æ·»åŠ æ–°å·¥å…·

1. åœ¨ `mcp_tools.py` ä¸­æ³¨å†Œæ–°å·¥å…·
2. åœ¨ `document_converter.py` ä¸­å®ç°å·¥å…·é€»è¾‘
3. æ›´æ–°APIæ–‡æ¡£

### æ”¯æŒæ–°LLMæä¾›å•†

1. åœ¨ `config.py` ä¸­æ·»åŠ æ–°æä¾›å•†é…ç½®
2. åœ¨ `llm_client.py` ä¸­å®ç°æ–°æä¾›å•†æ¥å£
3. æ›´æ–°æ–‡æ¡£

## ğŸ“ å¼€å‘

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç æ ¼å¼åŒ–
black .

# ç±»å‹æ£€æŸ¥
mypy .
```

### è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶
