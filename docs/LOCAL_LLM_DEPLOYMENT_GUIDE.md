# æœ¬åœ°LLM + MCPåè®®éƒ¨ç½²æŒ‡å—

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

DocuSynapseç°åœ¨æ”¯æŒé€šè¿‡MCPï¼ˆModel Context Protocolï¼‰åè®®é›†æˆæœ¬åœ°éƒ¨ç½²çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å®Œå…¨æœ¬åœ°åŒ–çš„æ–‡æ¡£è½¬æ¢æœåŠ¡ã€‚

### æ ¸å¿ƒç»„ä»¶
- **MCPæœåŠ¡å™¨**: å¤„ç†æ–‡æ¡£è½¬æ¢è¯·æ±‚å’ŒLLMé€šä¿¡
- **MCPå®¢æˆ·ç«¯**: å‰ç«¯ä¸MCPæœåŠ¡å™¨çš„é€šä¿¡æ¥å£
- **æœ¬åœ°LLMç®¡ç†å™¨**: ç®¡ç†Ollamaã€LM Studioç­‰æœ¬åœ°LLMæœåŠ¡
- **æ–‡æ¡£è½¬æ¢å·¥å…·**: åŸºäºAIçš„æ™ºèƒ½æ–‡æ¡£è½¬æ¢

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. å®‰è£…æœ¬åœ°LLMæœåŠ¡

#### é€‰é¡¹A: Ollama (æ¨è)
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# ä¸‹è½½å®‰è£…åŒ…: https://ollama.ai/download
```

#### é€‰é¡¹B: LM Studio
```bash
# ä¸‹è½½LM Studio: https://lmstudio.ai/
# æ”¯æŒWindowsã€macOSã€Linux
```

### 2. ä¸‹è½½å’Œå¯åŠ¨æ¨¡å‹

#### ä½¿ç”¨Ollama
```bash
# ä¸‹è½½æ¨èæ¨¡å‹
ollama pull llama3.1:8b        # 8GB RAM
ollama pull qwen2.5:7b         # ä¸­æ–‡æ”¯æŒ
ollama pull gemma2:9b          # Googleæ¨¡å‹
ollama pull codellama:7b       # ä»£ç ä¸“ç”¨

# å¯åŠ¨æœåŠ¡
ollama serve
```

#### ä½¿ç”¨LM Studio
1. æ‰“å¼€LM Studio
2. åœ¨"Models"é¡µé¢ä¸‹è½½æ¨¡å‹
3. åœ¨"Local Server"é¡µé¢å¯åŠ¨æœåŠ¡
4. é»˜è®¤ç«¯å£: 1234

### 3. éƒ¨ç½²MCPæœåŠ¡å™¨

#### åˆ›å»ºMCPæœåŠ¡å™¨è„šæœ¬
```python
# mcp_server.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import json
import asyncio
from typing import Dict, Any

app = FastAPI(title="DocuSynapse MCP Server")

# å…è®¸è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# MCPå·¥å…·æ³¨å†Œ
TOOLS = {
    "convert_document": {
        "name": "convert_document",
        "description": "Convert document between formats with AI assistance",
        "inputSchema": {
            "type": "object",
            "properties": {
                "file_path": {"type": "string", "description": "Path to the input file"},
                "target_format": {"type": "string", "enum": ["pdf", "docx", "txt", "html"]},
                "preserve_formatting": {"type": "boolean", "default": True},
                "extract_tables": {"type": "boolean", "default": True},
                "extract_images": {"type": "boolean", "default": True},
                "language": {"type": "string", "default": "auto"}
            },
            "required": ["file_path", "target_format"]
        }
    }
}

@app.post("/mcp")
async def handle_mcp_request(request: Dict[str, Any]):
    """å¤„ç†MCPåè®®è¯·æ±‚"""
    try:
        method = request.get("method")
        params = request.get("params", {})
        request_id = request.get("id")
        
        if method == "tools/list":
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "result": {"tools": list(TOOLS.values())}
            }
        
        elif method == "tools/call":
            tool_name = params.get("name")
            tool_args = params.get("arguments", {})
            
            if tool_name == "convert_document":
                result = await convert_document_tool(tool_args)
                return {
                    "jsonrpc": "2.0",
                    "id": request_id,
                    "result": {"content": result}
                }
            else:
                raise HTTPException(status_code=400, detail="Unknown tool")
        
        else:
            raise HTTPException(status_code=400, detail="Unknown method")
    
    except Exception as e:
        return {
            "jsonrpc": "2.0",
            "id": request.get("id"),
            "error": {
                "code": -32603,
                "message": str(e)
            }
        }

async def convert_document_tool(args: Dict[str, Any]) -> Dict[str, Any]:
    """æ–‡æ¡£è½¬æ¢å·¥å…·å®ç°"""
    file_path = args["file_path"]
    target_format = args["target_format"]
    
    # è¿™é‡Œå®ç°æ–‡æ¡£è½¬æ¢é€»è¾‘
    # 1. è¯»å–æ–‡ä»¶
    # 2. è°ƒç”¨æœ¬åœ°LLMè¿›è¡Œå†…å®¹åˆ†æ
    # 3. ç”Ÿæˆç›®æ ‡æ ¼å¼æ–‡ä»¶
    
    return {
        "success": True,
        "data": "è½¬æ¢åçš„å†…å®¹",
        "fileName": f"converted.{target_format}",
        "confidence": 0.9
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "mcp-server"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### å®‰è£…ä¾èµ–
```bash
pip install fastapi uvicorn python-multipart
```

#### å¯åŠ¨MCPæœåŠ¡å™¨
```bash
python mcp_server.py
```

### 4. é…ç½®å‰ç«¯åº”ç”¨

#### æ›´æ–°ç¯å¢ƒå˜é‡
```bash
# .env.local
VITE_MCP_SERVER_URL=http://localhost:8000
VITE_LOCAL_LLM_ENABLED=true
```

#### é›†æˆMCPå®¢æˆ·ç«¯
```typescript
// åœ¨App.tsxä¸­æ·»åŠ æœ¬åœ°LLMæ”¯æŒ
import { MCPClient } from './services/mcpClient';
import { LocalLLMManager } from './services/localLLMManager';

const mcpClient = new MCPClient({
  serverUrl: process.env.VITE_MCP_SERVER_URL || 'http://localhost:8000'
});

const llmManager = new LocalLLMManager();
```

## ğŸ”§ é…ç½®é€‰é¡¹

### 1. Ollamaé…ç½®
```yaml
# ~/.ollama/config.json
{
  "host": "0.0.0.0:11434",
  "models": {
    "llama3.1:8b": {
      "temperature": 0.1,
      "max_tokens": 4000
    }
  }
}
```

### 2. LM Studioé…ç½®
- ç«¯å£: 1234
- æ¨¡å‹è·¯å¾„: è‡ªå®šä¹‰
- å†…å­˜é™åˆ¶: æ ¹æ®ç³»ç»Ÿè°ƒæ•´

### 3. MCPæœåŠ¡å™¨é…ç½®
```python
# config.py
MCP_CONFIG = {
    "host": "0.0.0.0",
    "port": 8000,
    "llm_endpoint": "http://localhost:11434",  # Ollama
    "timeout": 30,
    "max_file_size": "100MB"
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©å»ºè®®
| ç”¨é€” | æ¨èæ¨¡å‹ | RAMè¦æ±‚ | æ€§èƒ½ |
|------|----------|---------|------|
| æ–‡æ¡£è½¬æ¢ | llama3.1:8b | 8GB | ä¼˜ç§€ |
| ä¸­æ–‡æ–‡æ¡£ | qwen2.5:7b | 8GB | ä¼˜ç§€ |
| ä»£ç å¤„ç† | codellama:7b | 8GB | ä¼˜ç§€ |
| è½»é‡çº§ | gemma2:9b | 6GB | è‰¯å¥½ |

### 2. ç³»ç»Ÿè¦æ±‚
- **æœ€ä½é…ç½®**: 8GB RAM, 4æ ¸CPU
- **æ¨èé…ç½®**: 16GB RAM, 8æ ¸CPU, 8GB VRAM
- **å­˜å‚¨ç©ºé—´**: 20GB+ (æ¨¡å‹æ–‡ä»¶)

### 3. æ€§èƒ½è°ƒä¼˜
```bash
# Ollamaæ€§èƒ½ä¼˜åŒ–
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_FLASH_ATTENTION=1

# GPUåŠ é€Ÿ (å¦‚æœæœ‰NVIDIA GPU)
export CUDA_VISIBLE_DEVICES=0
```

## ğŸ”’ å®‰å…¨è€ƒè™‘

### 1. ç½‘ç»œå®‰å…¨
- MCPæœåŠ¡å™¨ä»…ç›‘å¬æœ¬åœ°æ¥å£
- ä½¿ç”¨é˜²ç«å¢™é™åˆ¶å¤–éƒ¨è®¿é—®
- è€ƒè™‘ä½¿ç”¨HTTPS (ç”Ÿäº§ç¯å¢ƒ)

### 2. æ•°æ®éšç§
- æ‰€æœ‰å¤„ç†åœ¨æœ¬åœ°å®Œæˆ
- ä¸å‘å¤–éƒ¨æœåŠ¡å‘é€æ•°æ®
- ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†

### 3. è®¿é—®æ§åˆ¶
```python
# æ·»åŠ APIå¯†é’¥éªŒè¯
@app.middleware("http")
async def verify_api_key(request: Request, call_next):
    api_key = request.headers.get("X-API-Key")
    if api_key != "your-secret-key":
        raise HTTPException(status_code=401, detail="Invalid API key")
    return await call_next(request)
```

## ğŸ› æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### OllamaæœåŠ¡æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# é‡å¯æœåŠ¡
ollama serve
```

#### æ¨¡å‹ä¸‹è½½å¤±è´¥
```bash
# æ¸…ç†ç¼“å­˜
ollama rm llama3.1:8b
ollama pull llama3.1:8b
```

#### MCPæœåŠ¡å™¨è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹æ—¥å¿—
tail -f mcp_server.log
```

### 2. æ—¥å¿—è°ƒè¯•
```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('mcp_server.log'),
        logging.StreamHandler()
    ]
)
```

## ğŸ“ˆ ç›‘æ§å’Œç»´æŠ¤

### 1. å¥åº·æ£€æŸ¥
```bash
# æ£€æŸ¥Ollama
curl http://localhost:11434/api/tags

# æ£€æŸ¥MCPæœåŠ¡å™¨
curl http://localhost:8000/health

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
ollama list
```

### 2. æ€§èƒ½ç›‘æ§
```python
# æ·»åŠ æ€§èƒ½ç›‘æ§
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} took {end_time - start_time:.2f} seconds")
        return result
    return wrapper
```

### 3. è‡ªåŠ¨é‡å¯
```bash
# ä½¿ç”¨systemd (Linux)
sudo systemctl enable ollama
sudo systemctl enable mcp-server

# ä½¿ç”¨PM2 (Node.js)
pm2 start mcp_server.py --name mcp-server
pm2 save
pm2 startup
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ¨¡å‹ç®¡ç†**: åªä¸‹è½½éœ€è¦çš„æ¨¡å‹ï¼Œå®šæœŸæ¸…ç†
2. **èµ„æºç›‘æ§**: ç›‘æ§å†…å­˜å’ŒCPUä½¿ç”¨ç‡
3. **å¤‡ä»½é…ç½®**: å®šæœŸå¤‡ä»½æ¨¡å‹å’Œé…ç½®æ–‡ä»¶
4. **ç‰ˆæœ¬æ§åˆ¶**: ä½¿ç”¨Dockerå®¹å™¨åŒ–éƒ¨ç½²
5. **æ—¥å¿—ç®¡ç†**: è®¾ç½®æ—¥å¿—è½®è½¬å’Œæ¸…ç†ç­–ç•¥

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„æœ¬åœ°LLM + MCPéƒ¨ç½²æ–¹æ¡ˆï¼ŒDocuSynapseå¯ä»¥å®ç°å®Œå…¨æœ¬åœ°åŒ–çš„AIæ–‡æ¡£è½¬æ¢æœåŠ¡ï¼Œç¡®ä¿æ•°æ®éšç§å’Œå®‰å…¨ï¼
